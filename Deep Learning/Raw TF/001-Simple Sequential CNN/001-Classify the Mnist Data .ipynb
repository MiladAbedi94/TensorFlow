{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "(Train_Imgs, Train_Labels), (Test_Imgs, Test_Labels) = mnist.load_data()\n",
    "Train_Imgs=Train_Imgs/255\n",
    "Test_Imgs=Test_Imgs/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The train data set contains {} images with the size of {} * {}.\".format(*Train_Imgs.shape))\n",
    "print(\"The test data set contains {} images with the size of {} * {}.\".format(*Test_Imgs.shape))\n",
    "print(\"Here are some sample data point:\\n\")\n",
    "f, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "Sample_Indexes=[np.where(Train_Labels==i)[0][0] for i in range(10)]\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        axes[i,j].imshow(Train_Imgs[Sample_Indexes[i*5+j]])\n",
    "        axes[i,j].grid(False)\n",
    "        axes[i,j].set(xlabel=\"Label={}\".format(i*5+j))\n",
    "        axes[i,j].set_yticklabels([])\n",
    "        axes[i,j].set_xticklabels([])\n",
    "        axes[i,j].tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, left=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The distribution of the labels both for the train data set, and for the test data set is uniform.\\n\")\n",
    "print(\"The data set does not suffer from class imbalance, or shift in distribution from train to test.\\n\")\n",
    "f, axes = plt.subplots(1, 2, figsize=(14, 7), sharex=True)\n",
    "sns.set(font_scale=2.5)\n",
    "sns.distplot(Train_Labels,kde=False,bins=np.array(range(11))-0.5,ax=axes[0],color=\"green\");\n",
    "sns.distplot(Test_Labels,kde=False,bins=np.array(range(11))-0.5,ax=axes[1],color='red');\n",
    "axes[0].set(xlabel=\"Label (Train Data)\",ylabel=\"Frequency\");\n",
    "axes[1].set(xlabel=\"Label (Test Data)\",ylabel=\"Frequency\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MiladAbedi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "Label_Layer=tf.placeholder(dtype=tf.uint8,name=\"Label_Layer\")\n",
    "Onehot_Label=tf.one_hot(Label_Layer,depth=10,dtype=tf.float32)\n",
    "\n",
    "Input_Layer=tf.placeholder(dtype=tf.float32,shape=[None,28,28],name=\"Input_Layer\")\n",
    "Expanded_Input=tf.expand_dims(Input_Layer,-1,\"Expanded_for_Channel\")\n",
    "\n",
    "First_Conv=tf.layers.Conv2D(filters=128,kernel_size=(5,5),activation='relu',strides=1,name=\"First_Conv\")(Expanded_Input)\n",
    "First_Maxpool=tf.layers.MaxPooling2D(pool_size=(2,2),strides=2,name=\"First_Maxpool\")(First_Conv)\n",
    "\n",
    "Second_Conv=tf.layers.Conv2D(filters=64,kernel_size=(5,5),activation='relu',strides=1,name=\"Second_Conv\")(First_Maxpool)\n",
    "Second_Maxpool=tf.layers.MaxPooling2D(pool_size=(2,2),strides=2,name=\"Second_Maxpool\")(Second_Conv)\n",
    "\n",
    "Third_Conv=tf.layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',strides=1,name=\"Third_Conv\")(Second_Maxpool)\n",
    "Third_Maxpool=tf.layers.MaxPooling2D(pool_size=(2,2),strides=2,name=\"Third_Maxpool\")(Third_Conv)\n",
    "\n",
    "Flattener=tf.layers.Flatten(name=\"Flattener\")(Third_Maxpool)\n",
    "First_Dense=tf.layers.Dense(units=10,activation=None,name=\"First_Dense\")(Flattener)\n",
    "\n",
    "\n",
    "Loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=First_Dense, labels=Onehot_Label))\n",
    "Predict=tf.dtypes.cast(tf.math.argmax(First_Dense,axis=-1),dtype=tf.uint8)\n",
    "\n",
    "Metric=tf.reduce_mean(tf.dtypes.cast(tf.math.equal(Label_Layer,Predict),dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer=tf.train.AdamOptimizer()\n",
    "Task=Optimizer.minimize(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.162 2.2739704 [0 1 2 4 6 8]\n",
      "0.738 1.2385957 [0 1 2 3 4 6 7 8 9]\n",
      "0.872 0.38452283 [0 1 2 3 4 5 6 7 8 9]\n",
      "0.962 0.15491429 [0 1 2 3 4 5 6 7 8 9]\n",
      "0.992 0.053570703 [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "Loss_List=[]\n",
    "Prediction=[]\n",
    "Accuracy=[]\n",
    "FD=[]\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        Feed_Dict={Input_Layer:Train_Imgs[:500],Label_Layer:Train_Labels[:500]}\n",
    "        session.run(Task,feed_dict=Feed_Dict)\n",
    "       # Loss_List.append(Loss.eval(feed_dict=Feed_Dict))\n",
    "        #Prediction.append(Predict.eval(feed_dict=Feed_Dict))\n",
    "        #FD.append(First_Dense.eval(feed_dict=Feed_Dict))\n",
    "        if i%10==0:\n",
    "            temp=session.run([Metric,Loss,Predict],feed_dict=Feed_Dict)\n",
    "            print(temp[0],temp[1],np.unique(temp[2]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(FD[-1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Labels[65:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = tf.layers.conv2d(inputs = img, filters = nfil1,\n",
    "                          kernel_size = ksize1, strides = 1,\n",
    "                          padding = \"same\", activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models,layers,utils\n",
    "Model=models.Sequential()\n",
    "Model.add(layers.Conv2D(filters=128,kernel_size=(5,5),input_shape=(28,28,1),activation='relu',strides=1,name=\"First_Conv\"))\n",
    "Model.add(layers.MaxPooling2D((2,2),name=\"First_Maxpool\"))\n",
    "Model.add(layers.Conv2D(filters=64,kernel_size=(5,5),activation='relu',strides=1,name=\"Second_Conv\"))\n",
    "Model.add(layers.MaxPooling2D((2,2),name=\"Second_Maxpool\"))\n",
    "Model.add(layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',strides=1,name=\"Third_Conv\"))\n",
    "Model.add(layers.MaxPooling2D((2,2),name=\"Third_Maxpool\"))\n",
    "Model.add(layers.Flatten(name=\"Flattener\"))\n",
    "Model.add(layers.Dense(units=10,activation='softmax',name=\"Fist_Dense\"))\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Imgs = Train_Imgs.reshape((-1, 28, 28, 1))\n",
    "Test_Imgs=Test_Imgs.reshape((-1,28,28,1))\n",
    "Train_Labels = utils.to_categorical(Train_Labels)\n",
    "Test_Labels = utils.to_categorical(Test_Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "Model.fit(Train_Imgs, Train_Labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.log(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
